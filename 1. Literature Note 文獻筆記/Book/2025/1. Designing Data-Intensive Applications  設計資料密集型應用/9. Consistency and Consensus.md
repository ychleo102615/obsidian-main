---
tags: 
date: 2025-05-18
time: 14:07
---

在分散式系統中，一個最重要的抽象就是consensus：讓所有的node同意一件事。


## Consistency Guarantees

eventual consistency（或稱convergence）是弱的保證，他不保證你要等多久時間。

## Linearizability
在最終一致性資料庫中，同時向兩個節點讀取資料，可能會得到不同的結果。

atomic consistency, strong consistency immediate consistency, external consistency

### What makes a system linearizable
讀取第一次之後的其他所有讀取也必須要是同一個值。
serializable snapshot isolation不是linearizable。

### Relying on Linearizability

#### Locking and leader election
single-leader replication中，lock的實作方式得是linearizable。所有的node必須同意一某個node得到了lock。
Oracle RAC 這類分散式資料庫，為了在共享儲存架構下實現細粒度同步與一致性，會對每個磁碟頁加上 linearizable lock，並使用專用高速網路來避免這些鎖變成效能瓶頸。


#### Constraints and uniqueness guarantees
保證username, email的唯一性。
有點像lock。

#### Cross-channel timing dependencies
違反linearizability的時候，代表還有另一個管道可以收到通知（書中的足球比賽，Bob透過Alice的口頭通知）

上傳圖片的服務，有兩個管道。一個要儲存圖片到file storage，一個要排隊進message queue。有可能排隊到了但檔案沒好。

### Implementing Linearizable Systems
提到single-leader, consensus algorithm, mulit-leader, leaderless

#### Linearizability and quorums
最好還是假設leaderless system不保證 linearizability

### The Cost of Linearizability

#### The CAP theorem
#### Linearizability and network delays

## Ordering Guarantees

- single leader的架構可以決定寫入的順序
- Serializability。透過真的依序執行或是避免序列衝突的方式達成（lock, abort）
- 時間戳記

### Ordering and Causality
- [[5. Replication#Consistent Prefix Reads]]  問與答是有因果相依的
- [[5. Replication#Detecting Concurrent Writes]] 交易得先知道彼此才有可能相依
- [[7. Transactions#Snapshot Isolation and Repeatable Read]] 可視範圍的資料都符合causality（防止read skew）
- [[7. Transactions#Write Skew and Phantoms]] 不是資料本身，而是查詢條件改變時發生。[[7. Transactions#Serializable Snapshot Isolation (SSI)|Serializable Snapshot Isolation]] 可以偵測
- [[#Cross-channel timing dependencies]]

有按照因果排序的話，我們稱之因果一致性 causality consistency

#### The causal order is not a total order

*Linearizability*
任意兩個交易可以比較先後順序

*Causality* 
併發的資料無法互相比較

#### Linearizability is stronger than causal consistency

有線性一致性的話就一定有因果一致性。但我們不見得要用線性一致性去保證因果一致性。

因果一致性其實是在不易受延遲影響的模型中最強的一致性了。

#### Capturing causal dependencies

[[5. Replication#Version vectors]]
資料庫會記住哪些資料被哪些交易讀取

### Sequence Number Ordering
如果我們要記住所有交易讀取過的內容，那會是不小的開銷。
我們可以使用logical clock的時間戳記 [[8. The Trouble with Distributed Systems#Monotonic clocks]]

#### Noncausal sequence number generators
在多個節點的話會比較不好生成序列。以下是一些方法：
- 在兩個節點個別使用奇數偶數來遞增序列
- 使用時間戳記（[[8. The Trouble with Distributed Systems#Timestamps for ordering events]]）
- 事先規劃，節點A使用 1 ~ 1000，節點B使用 1001 ~ 2000

但是這些方法無法正確地捕捉到彼此的先後順序

#### Lamport timestamps
蘭伯特時間戳
時間戳是node id, logical counter 的元組
除了counter，還有node id 可以比較

Node, client都會記憶目前最大的counter，如果node發現目前request的counter更大時，就會更新它。

 與version vector比較，Lamport timestamps 比較精簡(compact)
#### Timestamp ordering is not sufficient
時間戳的方式只有在「事後」可以有效比對，但像是註冊username這種usecase 會需要當下就回傳錯誤。

靠total order broadcast

### Total Order Broadcast
如果是single-leader的話，本來就是total order。挑戰來自當single-leader不足以應付擴展，或是故障轉移的時候。

Total Order Broadcast通常被描述成一個協議，他需要滿足這兩個特性
##### Reliable delivery
如果一個訊息被送到其中一個節點，那每個節點都必須要收到那個訊息。
##### Totally ordered delivery
每個節點，訊息會被同樣的順序送達。

total order broadcast 也被稱為 atomic broadcast

#### Using total order broadcast
#### Implementing linearizable storage using total order broadcast

total order與linearizability的不同之處：前者講求訊息的順序，訊息可以是延遲的。線性一致性則要求讀取一定要是最新的寫入。

> [!NOTE]
> **Total Order Broadcast 是保證「所有節點收到的訊息順序一致」的通訊抽象，**  
> 它是實作分散式一致性（如複製、交易排序、共識）的基礎，常透過共識協議如 Paxos 或 Raft 來實作。
> 
> ## ✅ 實作 total order broadcast using linearizable storage
> 這段文字的關鍵流程如下：
> 1. **你有一個線性一致的整數儲存（register）**，支援：
>     - `increment-and-get()` 或 `compare-and-set()`
>     - 每次操作都會獲得一個唯一且單調遞增的整數（**全域序號**）
> 2. **每次要送出訊息時**：
>     - 呼叫 `increment-and-get()` → 拿到一個唯一序號
>     - 將訊息與這個序號一起廣播給其他節點
> 3. **所有節點依照這個序號排序並依序 deliver**
>     - 若先收到 `seq = 6`，但還沒看到 `seq = 5`，就要等
>     - ✅ 這樣就保證了所有節點 deliver 的順序一樣 → 就是 total order
> 	
> ✅ 注意這裡一個關鍵對比：
> > **Lamport timestamp 雖然可以部分排序，但有 gaps，不能保證所有節點收到的順序一致**

Consensus、Total Order Broadcast 與 Linearizability 是功能上等價的抽象，但分別聚焦在「決策」、「順序」、「操作語意」三種不同層級與角色的問題，這樣的語言區分讓我們在設計系統、定義 API、或理解演算法時能更清楚掌握關注點與設計責任。

# Distributed Transactions and Consensus

這些情境中，每個節點都需要對這些事情「一致性決議 consensus」

##### Leader election
一致性決議可以防止故障轉移出問題（造成split brain）
##### Atomic commit
如何讓多個參與者在沒有共用記憶體、且可能故障的情況下，對一個交易的「提交或回滾」做出一致決定？

### Atomic Commit and Two-Phase Commit (2PC)

#### From single-node to distributed atomic commit

我們有很多個節點都要做同一組交易，但是有的節點可能會失敗（constraint violation, conflict, lost in the network, crash before the commit）
 **節點**必須且唯有在**確定所有其他節點都會提交的情況下，才能提交這筆交易。**

#### Introduction to two-phase commit
保證所有節點提交或是中止

coordinator得先向所有節點問prepare，都是yes的話才可以commit。

> [!NOTE]
> #### A system of promises
> ## **概念：Two-Phase Commit 是一個「承諾協議」**
> - 它的本質：**先投票表態，之後再根據表態進行不可逆決策**
> - 每個節點在 prepare 階段不是直接提交，而是**承諾未來無論如何都會服從結果**
> 
>  Two-Phase Commit 保證原子性，不是因為它沒有風險，而是因為它透過兩個「不可回頭的承諾點」：節點投票 yes 代表 **無條件 commit 保證**，coordinator 決定後則 **必須執行到底**，這兩者的設計建構了一個「一致的承諾系統」。

#### Coordinator failure
當節點prepare之後，無論如何都要等coordinator。此時fail的話，節點就得一直等（uncertain）。

2PC協議中，我們得等到coordinator回復，其他節點才能繼續。
所以coordinator在commit之前，得先寫到transaction log再commit，出事才會回復。

### Distributed Transactions in Practice

##### Database-internal distributed transactions
參與交易的所有節點使用同一個資料庫軟體。

##### Heterogeneous distributed transactions
指的是跨越不同技術系統（例如資料庫 + 訊息系統）的分散式交易，這類交易仍需保證 atomic commit，但因為每個系統內部實作不一致，所以協調與容錯處理比同質系統更複雜。

#### Exactly-once message processing

#### XA transactions
eXtended Architecture，他只是一個與coordinator溝通的 C API。

#### Holding locks while in doubt

#### Recovering from coordinator failure
transaction有可能因為coordinator那邊出了bug，而變成「孤兒」，並且會一直卡在那裡。

這只能靠管理者手動處理。這需要檢查其他所有的participants的in-doubt transaction是被committed還是aborted。

有些實作會有叫做 *heuristic decisions* 的緊急出口。他不保證正確，只是用來迴避最慘狀況的手段。

#### Limitations of distributed transactions

- coordinator導致無法真正達到stateless model


### Fault-Tolerant Consensus
**分散式共識演算法 四大共識性質（以淺白語言解釋）**

| **條件名稱**              | **中文名稱** | **意義說明**                                                     |
| --------------------- | -------- | ------------------------------------------------------------ |
| **Uniform Agreement** | 一致性協議    | ❗ **不能有兩個節點決定了不同的值**。→ 所有達成共識的節點都應該決定一樣的結果。                  |
| **Integrity**         | 完整性      | ❗ **每個節點最多只能決定一次**。→ 不可以「改變主意」或決定兩次。                         |
| **Validity**          | 合法性      | ❗ **被決定的值，一定來自於某個節點原本提出的提案**。→ 共識不是隨機創造出來的，要有節點曾經提出它。        |
| **Termination**       | 終止性      | ❗ **所有沒有故障的節點，最終都會做出決定**。→ 不能無限等待或卡住。這是 **活性（liveness）** 保證。 |

#### Consensus algorithms and total order broadcast

#### Single-leader replication and consensus

就算single-leader本質上就滿足了total order broadcast，但是當leader失效時，需要手動指定下個leader，這樣就不滿足 termination。

#### Epoch numbering and quorums
面臨split brains時，有著比較大的epoch number的獲勝（每次選舉就會遞增的數字）。
這裡也會利用過半的majority來決定leader（第一個自願的participant最有可能成為新leader）。
這是single leader的重要概念。Raft有實踐。
#### Limitations of consensus

### Membership and Coordination Services
ZooKeeper and etcd這些特殊資料庫會把所有資料放在記憶體（也會為了能夠復原寫入磁碟）。
資料會透過fault-tolerant total order broadcast algorithm複製給所有節點。

##### Linearizable atomic operations
使用租約實踐lock 
##### Total ordering of operations
使用monotonically increasing transaction ID (zxid) and version number (cversion)作為fencing防止寫入衝突
##### Failure detection
用心跳包檢查
##### Change notifications
節點會彼此通知

雖然只有Linearizable atomic operations需要一致性決議，但正因為其他的特性才讓zookeeper 對distributed coordination如此有用。

#### Allocating work to nodes
協助single leader選舉
協助分區以及分區平衡rebalancing partitions

Zookeeper紀錄的資訊通常變動的很慢，通常以分鐘到小時來算。他記的資訊可能是分區7的leader在哪裡的這種資訊。

#### Service discovery
ZooKeeper, etcd, and Consul 通常都是用來找你要的服務up位址在哪。

他們通常不需要一致性決議，但是選leader的時候就要了。

#### Membership services
這個服務告訴大家：「現在這個系統中有哪些人在線上？」










[[2025-07-27|2025-07-27 Sun, 21:28]]
**Serializability 檢測依賴衝突，Linearizability 強制時序對應**