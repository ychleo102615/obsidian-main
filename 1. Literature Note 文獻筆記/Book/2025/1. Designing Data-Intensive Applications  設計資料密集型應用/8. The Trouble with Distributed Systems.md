---
tags: []
date: 2025-05-11
time: 18:04
---

## Faults and Partial Failures

if an internal fault occurs, we prefer a computer to crash completely rather than returning a wrong result, because wrong results are difficult and confusing to deal with.
內部錯誤發生時，我們希望系統倒下，而不是回傳錯誤的結果。

當你寫的軟體在多個電腦上運行，並且使用網路溝通，那相比一台電腦總是有確定的結果，會有很大的差別。

在分散式系統中，總是有一部分的系統以不可預測的方式壞掉，即使其他部分運作正常。
這被稱之為 *partial failure*。分散式系統最困難的地方在於，他不是總是一樣的（nondeterministic）。

當我們的系統透過多個節點和網路運作時，我們甚至無法知道他們運作有沒有成功，因為連傳遞訊息的時間都是隨機的。

### Cloud Computing and Supercomputing
在大型計算系統中，有兩個光譜：
超級電腦、日常雲端計算
目前一般企業的系統大多在這段光譜的某個位置上

超級電腦的使用情境像是一種single node的電腦，會在計算過程中紀錄checkpoint，以便之後回復。
而有部分系統出錯時，就直接視為整體崩潰。


- 一般線上服務，停下來維修是比較無法接受的。而線下的天氣預測則不會這麼嚴苛
- 一般的電腦比較容易壞，而超級電腦多為特殊訂製的
- 網路不同
- 當基數夠多時，幾乎可以保證系統隨時都會有壞掉的節點。如果因此就停擺的話，那就等同大部分的時間都沒在做工作了
- 如果我們能忍受失效節點的話，想要維護或升級時是很好用的
- 橫跨地理的網路很慢也很不可靠

**Building a Reliable System from Unreliable Components**

## Unreliable Networks

當我們沒有收到回覆時，我們無法知道他是因為網路導致中途遺失，還是對方失效。
甚至無法得知傳送有沒有成功。

### Network Faults in Practice
一個中等的資料中心，其內部每個月會發生12起網路故障。
- **資料中心的網路故障頻率其實比預期高很多**（每月 12 次不是小數目）
- **而且影響範圍可能從單台機器到整個 rack**  
    → 如果你的系統沒有考慮到「rack 級故障」，可能會誤判是多個機器同時掛掉

此外，單靠硬體備援（機架頂端交換器（top-of-rack switches）、匯聚交換器（aggregation switches）、以及負載平衡器（load balancers））不足以達到高可用性，因為很多故障根本來自於人為設定錯誤，而不是設備壞掉。

switch網路升級會造成delay。

鯊魚會咬斷電纜。

當出錯的機率不高時，直接通知使用者錯誤也是一個方法。但重點是你要知道你的軟體會怎麼應對這個錯誤，並從中回復。

### Detecting Faults

系統需要自動偵測故障的節點
- load balancer要跳過dead node
- single-leader 失效時，其中一個follower要成為新leader

分散式系統中「節點可達 ≠ 服務正常」，即使有 TCP 回應也不代表資料有被完整處理。
這就是為什麼在設計 **可容錯系統（fault-tolerant systems）** 時，**要用應用層協議去確認語意完整性（semantic success）**，而不能只依賴 TCP 的傳輸成功與否。

有時候node崩潰時，但該作業糸統還在執行的話，可以有一個腳本讓其他node來接手。

如果想要保證請求有成功，我們需要得到正面回應。
如果意外發生，我們可能會得到負面回應，或是什麼都得不到。

TCP會幫我們重傳請求，但我們也可以從應用層面上來處理。

**分散式系統中的「失敗偵測」永遠是模糊的（不準確、延遲、甚至錯誤）**


### Timeouts and Unbounded Delays

決定這個等待時間是個很難的問題。

太短的話，其他node會接手，但其實被判定為dead的節點只是還在計算途中的話，可能會導致操作兩次。
而且如果系統負載已經很高的話，此時減少node數量的做法很可能只會讓事情更糟。會像滾雪球般的讓所有節點都停止運作。

#### Network congestion and queueing
網路封包也會塞車
1. 如果switch已達容量上限，封包會被丟棄
2. 如果伺服器cpu已被佔滿，請求也會queue住
3. 虛擬環境中，目前作業系統常會停個10毫秒，cpu切換至其他作業系統。這導致這邊也有queue的空間
4. TCP 在資料送進網路前，也會queue

Tcp在時間內沒收到回覆的話，也會重傳。雖然應用程式不必擔心，但仍會影響delay。

UDP 在語音或是會議的使用情境會比tcp來得適合。他不處理重傳和flow control，因為資料的即時性更重要。

我們需要試驗性的嘗試不同的timeout，或是監控並因應調整。

### Synchronous Versus Asynchronous Networks

打電話是一種同步網路，一旦連線接通就會在兩個端點間佔住一個頻寬，並且不會有queue的問題。

因此，延遲是有上限的，稱之 *bounded delay*。

#### Can we not simply make network delays predictable?

現今網路之所以不用連線的方式傳資料，其中一個原因是相比電話這種穩定流量的使用情境，一般都是想在短時間內盡快傳送資料。例如讀取網頁、下載檔案。

目前實際部署的網路技術（例如 TCP/IP）無法保證延遲一定低、封包一定會送達。

延遲算是一種為了以較低成本取得基礎建設（網路或是cpu）所做的取捨。

網路是不可靠的，不可預測的，因此任何對 timeout、節點是否掛掉的判斷都只能是「猜測」，不能是絕對的事實。

## Unreliable Clocks

對於時間，我們有計算時長與時間點的兩種用法。

每個機器都有他自己的時間，我們通常用Network Time Protocol(NTP)來校準時間，而這些server的來源會更精準，例如gps receiver。

### Monotonic Versus Time-of-Day Clocks
#### Time-of-day clocks
回傳日期與時間的時鐘，通常是計算1970/1/1到現在過了多少秒。
通常也會用NTP同步，所以可能會有機器跑太快，同步時時間回退的問題。

#### Monotonic clocks
只會遞增的時鐘，相比前者更適合用來算經過的時間（elapsed time）。

### Clock Synchronization and Accuracy
硬體或是NTP的時間其實都能很善變
- 石英時鐘會隨硬體溫度改變更新頻率，最多一天可能差到17秒
- 若同步時，時間差太多，電腦可能會拒絕同步或是重置
- 如果 ntp 被防火牆擋住，電腦可能會長時間沒有同步
- 網路延遲也會影響同步
-  ntp server可能不可靠
- 系統如果沒有對leap seconds有應對的話，很多大型系統因此崩潰
- 虛擬機作業系統切換時，也會遇到時間突然往前的現象
- 手機或遊戲機，使用者可以自行設定時間

### Relying on Synchronized Clocks
很多潛在問題：一天的秒數不等同86400；時間忽前忽後；不同節點間的時差過大
穩固的系統必須準備好面對錯誤的時鐘。
這類問題常發生於我們不好察覺，即使時間偏差越來越多。問題也不是大崩潰，而是莫名少了一些資料。

當我們的系統需要同步的時鐘時，我們需要監測他們的時間，當一節點與其他人時差太多時，得將其移除，在他造成更大的傷害之前

#### Timestamps for ordering events

在multi-leader和leaderless架構中，我們常用 *last write wins* 的策略。但是當時間戳記不可靠時，這可能導致寫入遺失。

- 時鐘比較慢的節點無法覆寫來自時鐘比較快的節點。這種資料遺失甚至不會噴錯或有報告
- LWW 無法判斷併發的資料誰先誰後 [[5. Replication#Detecting Concurrent Writes]]
- 還有時間戳完全一樣的狀況

我們需要logical clocks 或是說 Version vectors 來判斷前後順序。

#### Clock readings have a confidence interval
不管怎樣，時間都不會是準確的。我們能合理推論的只有，時間差不多是在這個區間。

#### Synchronized clocks for global snapshots
快照隔離會需要一個遞增的交易ID。在單一node的資料庫中，簡單的計數器就夠用了。

但在分散式系統中的交易ID就不好產生了。
Spanner在資料中心用了以下技術：他們用TrueTimeAPI取得一個區間，我們會比較兩個交易的區間是否重疊。
Spanner甚至會故意等待一個信賴區間的長度，以避免重疊。為了降低這個等待時間，時鐘也要越精確越好。

#### Process Pauses
在single leader的使用場景中，要怎麼知道該leader還是leader（他還沒被其他node宣告陣亡）？
像是一個會過期的lock，我們可以給leader 「租約 *lease*」。
leader必須不斷延長租約。如果租約過期則代表他掛了，其他節點可以接手leader。
```java
while (true) {
	request = getIncomingRequest();
	// Ensure that the lease always has at least 10 seconds remaining
	if (lease.expiryTimeMillis - System.currentTimeMillis() < 10000) {
		lease = lease.renew();
	}
	if (lease.isValid()) {
		process(request);
	}
}
```
這段程式碼有幾個問題
1: expiryTimeMillis可能是別的node給的，卻與本地時間比較。
2: 雖然我們在開頭檢查有沒有到期，但是我們可能在處理request的中途過期

- Java的垃圾回收可能持續幾分鐘
- 虛擬機可能會被暫停
- 進入休眠模式
- 切換thread
- disk I/O
- swapping to disk(paging), thrashing
- 傳送`SIGSTOP`信號

以上情境接可以拖延執行緒，而其他人卻可能認為你不回應所以過期了。

#### Response time guarantees

*hard real time*  systems
像是飛機、火箭、機器人、汽車等操作物理物件的系統，必須要在可預測的時間內盡快回應，否則會有大災難。

*real-time operating system* RTOS 對其CPU、記憶體、library都會有限制，並藉由大量測試保證最低需求有被滿足。

大部分的程式語言和工具並沒有提供real-time guarantee。
所以要開發這種系統是很昂貴的，多用於safety-critical embedded devices。且不需要高性能，他們通常是低吞吐量但是高優先度的系統。

#### Limiting the impact of garbage collection

當一個Node需要GC的時候，就讓request導向其他的Node。
有另一種方式是只讓短週期物件，並在累積夠多長週期物件時重啟。類似於rolling update 的操作

## Knowledge, Truth, and Lies

分散式系統的節點，永遠無法確認網路送過來的資料是真實無誤的。

設想我們能做到的假設與能夠提供保證。

### The Truth Is Defined by the Majority
在分散式系統中，單一節點是不可靠的。取而代之，我們選擇去依賴 *quorum* ，一種投票機制。
參考leaderless replication

#### The leader and the lock
- 分區中，只有一個節點可以當leader
- 只有一個交易或是使用者可以持有lock，防止併發寫入
- 使用者ID的唯一性

這些在分散式系統中，沒有quorum的同意的話，負責操作的node可能就被宣判死亡。

因為[[#Process Pauses]]，取的lock的client可能沒意識到自己的租約已經到期，並寫入資料。此時如果其他人已經寫入的話將會導致衝突。

#### Fencing tokens
從上面的例子，我們在寫入中加入租約認證的token，檢查他是否過期。
由server來檢查傳入的資料是否合法是好的設計（不要信任client）。
這防止了[[inadvertently]]的client錯誤。


### Byzantine Faults

本書中我們假設節點不可靠但是 **誠實**。
如果一個節點宣稱他已經收到某個資料，但其實他還沒的話，我們稱之 **拜占庭將軍問題**。

在航空領域，CPU暫存器可能因為電磁波而產生不可預測的變化（這攸關人命），所以飛行控制系統必須要能處理拜占庭問題。
比特幣交易也是。

現實中要處理這種問題，在經濟考量上面是不實際的。

網路應用，透過input validation、消毒、output escaping等操作，防止向SQL injection, crosssite scripting。

Bug也可被視為拜占庭錯誤，但如果你都把它配置出去的話，Byzantine fault-tolerant algorithm也無法救你。
也沒有一種protocol可以防止這種問題。如果攻擊者可以攻下一個node的話，他大概也能攻下其他所有node了。

所以目前主要還以傳統的方式（認證、防火牆、加密...）保護系統免於攻擊者。

#### Weak forms of lying
硬體問題、軟體bug、錯誤配置等問題可能會造成「弱」說謊。

- check sums 在tcp和應用的層級上
- 限制數值範圍、字串長度
- NTP使用多個server，計算多數同意的區間

### System Model and Reality
定義系統模型來為可能發生的錯誤確立形式

##### *Synchronous model*
假設有限範圍的網路延遲、程序暫停（process pause）、時鐘錯誤。
這並是指完全的同步時鐘或是零網路延遲，只是說你知道這些事情存在，而且不會超過特定上線。

這個模型並不實際，因為大部分的系統都有unbounded delays 和 pauses 。

##### *Partially synchronous model*
大部分的時候都像synchronous model，偶爾會延遲、暫停。
我們需要檢查，時間相關的前提假設有可能會壞掉。

##### *Asynchronous model*
系統不允許建立時間假設。

除了時間問題，也有節點失效的問題。
##### *Crash-stop faults*
節點只會因為崩潰(crashing)而失效。而且不會復原。

##### *Crash-recovery faults*
節點隨時會崩潰，也許過了一陣子之後會回復。
這種模型，節點回有穩定的存儲stable storage，即使崩潰也不會遺失。

##### *Byzantine arbitrary faults*
節點可能會互相欺騙。

最普遍的就是 partially synchronous model + crash-recovery faults 模型。


#### Correctness of an algorithm

##### *Uniqueness*
沒有任何兩個請求fencing token的結果會是一樣的

##### *Monotonic sequence*
token依序遞增

##### *Availability*
有請求fencing token且沒有崩潰的節點總會收到回覆 response。

一個分散式演算法的正確性，只在**其所依賴的系統模型假設下才有意義**。如果發生的是模型外的情況（如所有節點都 crash），那演算法行不行根本不是它該負責的事。

#### Safety and liveness
- safety property：如果他被違反了，我們能夠知道這什麼時候發生的，並且無法修復
- liveness property：他可能會暫時失效，但是最終會被滿足

#### Mapping system models to the real world
系統模型提供我們設計與證明演算法正確性的工具，但現實世界會發生模型以外的錯誤，因此實作時仍需面對不可預測性。理論與實務，缺一不可。