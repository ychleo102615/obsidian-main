---
tags: []
date: 2025-06-04
time: 23:33
---

## Data Integration
不會有一種軟體適合處理所有的情境。一切都會有所取捨。一開始就想把所有事情做到好的設計，幾乎可以保證他會實作得很差。

### Combining Specialized Tools by Deriving Data
#### Reasoning about dataflows
#### Derived data versus distributed transaction

|   |   |   |
|---|---|---|
|特性|Distributed Transactions|Log-based Derived Data|
|一致性|線性一致性（linearizability）|最終一致性（eventual consistency）|
|可讀性保障（如RYOW）|有|無（需額外處理）|
|可擴展性與效能|較差|較佳|
|容錯性|差（尤其是XA）|較好（可重放log）|
|實作複雜度|難（分布式鎖 + 兩階段提交）|難（需考慮 idempotence, 重試, 延遲）|
##### 那該選哪一個？
•	如果你需要 強一致性（如銀行轉帳），並願意為此付出效能與開發代價 → 選 分散式交易。
•	如果你追求 效能與彈性，並能容忍短暫的不一致（如搜尋、推薦系統）→ 選 衍生資料同步。

#### The limits of total ordering
「全序事件處理」適合小型、集中式系統，但在大規模、分散式架構中，維持全序會遇到效能與設計上的瓶頸，因此實務上往往會退而求其次，改用局部順序（partial order）來處理。

#### Ordering events to capture causality
解除朋友關係 -> 傳送訊息給朋友
這兩個事件彼此之間有因果關係

> [!NOTE]
> 
> ## 這個問題是「跨資料來源的 join 時機不一致」
> 這跟之前 DDIA 第十一章講的 **stream-stream join** 和 **time-dependent joins** 有關。你必須小心：
> - **事件順序可能亂掉**
> - **某些事件實際上應該等別的事件處理完後才處理**
> 
> ## 🛠 有哪些解法可以考慮？
> 1. 使用 **邏輯時戳（Logical Timestamps）**
>     - 例如：Lamport clocks, vector clocks。
>     - 這樣你可以**標記事件的發生順序**，讓 consumer 判斷該不該處理。
>     - 問題是：仍然需要額外 metadata 傳遞，而且要小心處理 out-of-order delivery。
> 2. 明確記錄**用戶決策時所見的狀態**
>     - 例如在事件裡帶一個「前狀態的 snapshot id」，讓後續事件參照它。
>     - 就像：「我發這個訊息時，是在我不是他好友的狀況下」。
> 3. 使用 **衝突處理算法（Conflict Resolution）**
>     - 對「狀態型資料」（例如：好友列表）比較有效。
>     - 但對於有副作用的行為（例如：送通知），**無法補救錯誤**。
> 
> ## ✨ 小結
> 
>
>
> | 問題                   | 傳統方法                                          | 問題點          |
> | -------------------- | --------------------------------------------- | ------------ |
> | 事件順序搞錯導致邏輯錯誤         | 建立 total order / 全部集中處理                       | 太慢、擴展性差      |
> | 讓不同資料源一起決定邏輯行為（join） | 串流 join / 同步更新                                | 時間敏感，順序可能錯誤  |
> | 解法                   | 時戳 + metadata / 事件參照前狀態 / conflict resolution | 各有 trade-off |
> 


### Batch and Stream Processing
#### Maintaining derived state
分區、非同步的系統比較可靠及可擴展
#### Reprocessing data for application evolution
以前火車軌道寬度為了要統一，會增設第三條軌道，讓不同寬度的火車都可以使用，直到所有火車都換成統一寬度就可以廢除舊寬度的軌道了。
Reprocessing與這個道理很像。

#### The lambda architecture
#### Unifying batch and stream processing

## Unbundling Databases
比較Unix 和 Relational Database 這兩種資料管理系統。

### Composing Data Storage Technologies
資料庫的一些特色功能
- secondary index 
- materialized view
- replications log
- full-text search indexes

#### Creating an index

這個過程與與「建立 follower replica」與「streaming system 中的 CDC 初始快照」很類似。

> [!詳細比較]-
> ## 🔄 三種操作的共同點：從現有資料導出新視圖
> 
> |概念|描述|
> |---|---|
> |**CREATE INDEX**|從主表的欄位中擷取資料，排序並建構索引，以利日後查詢加速。|
> |**Follower Replica**|從 leader 節點取得一致快照，建立副本，再追上之後的變更。|
> |**CDC Snapshot**|資料流系統中，先從資料庫快照出發，然後開始接收變更紀錄（changelog）。|
> 
> 這三種操作的**核心過程都包含：**
> 
> 1. 建立某種**一致性快照（consistent snapshot）**
>     
> 2. **處理快照期間發生的 backlog**
>     
> 3. 開始持續**追蹤後續變動**
>     
> 
> ---
> 
> ## 🧠 精神本質：這些操作都是「衍生資料（Derived Data）」的一種形式
> 
> |類型|衍生資料的角色|本體與衍生關係|
> |---|---|---|
> |Index（索引）|加速查詢|原始資料 → 查詢優化視圖|
> |Replica（副本）|容錯或讀寫分離|原始資料 → 同步副本|
> |CDC Streaming|為其他系統提供即時資料變化通知|原始資料 → 變更事件流|
> 
> ---
> 
> ## 🔍 功能與行為上的異同比較
> 
> |特性|CREATE INDEX|Follower Replica|CDC Initial Snapshot|
> |---|---|---|---|
> |是否基於快照|✅ 是|✅ 是|✅ 是|
> |是否需要處理 backlog|✅ 是|✅ 是|✅ 是|
> |是否持續追蹤後續變更|✅（索引需即時更新）|✅（追隨 leader log）|✅（訂閱 changelog）|
> |建立後是否為即時視圖|✅ 是（幾乎同步）|✅ 是（視同步策略而定）|✅ 是（若串流不中斷）|
> |建立時是否會造成讀寫阻塞|❌ 可避開（使用 MVCC 建快照）|❌ 通常不中斷|❌ 非同步快照|
> |本體與視圖資料格式是否相同|❌（資料變成排序索引結構）|✅（與原資料一致）|❌（變為 append-only 的事件格式）|
> 
> ---
> 
> ## 💡 小結：核心相同，目標不同
> 
> - **目的差異：**
>     
>     - Index：為查詢效率服務
>         
>     - Replica：為容錯或讀擴展服務
>         
>     - CDC：為資料同步、事件驅動架構服務
>         
> - **操作相似：**
>     
>     - 都要從**現有的資料建立衍生結構**
>         
>     - 都需考慮 **資料一致性**、**資料處理順序**
>         
>     - 都處理兩階段：**初始化快照 + 後續同步**


#### The meta-database of everything
| 比喻                     | 說明                               |
| ---------------------- | -------------------------------- |
| 整個企業 = 一個超大資料庫         | 每個工具像資料庫的功能模組在外部執行               |
| Federated DB = 把查詢整合起來 | 多個資料來源像是不同的 table，透過 SQL 一起查     |
| Unbundled DB = 把寫入整合起來 | 事件（event）就是資料的同步單位，各系統訂閱自己該處理的部分 |
> **Federated DB 是一種統一讀取資料的方式，Unbundled DB 是一種統一發送更新的方式。**
- 前者適合查詢整合
- 後者適合處理整合（事件通知、資料同步等）

#### Making unbundling work
一般問題比較可能在寫入，也就是Unbundled的部分。
使用event log，而不是distributed transaction，有個很大的好處就是actors不會彼此耦合。

transaction中，有node失效的話，會影響全部參與的actors。但event logs就是彼此獨立，失效的節點可以之後自己再來catch up。

#### Unbundled versus integrated systems
有需要的時候，再使用Unbundled 架構。如果簡單的工具夠用，那就不要選擇複雜的。

### Designing Applications Around Dataflow
**“資料流驅動設計（Dataflow-oriented Design）”** 的應用程式開發模式

|類型|說明|類比|
|---|---|---|
|傳統式系統|寫入資料後，有人來查詢它。所有更新、聚合都靠人手動處理|每次 Excel 改資料後，你要手動更新報表或圖表|
|資料流式系統|寫入資料後，會主動推動後續處理邏輯、索引更新、檢查邏輯等|Excel 自動重新計算依賴公式，不需人工介入|

> 📌 **作者的觀點：**  
> 我們應該學會把應用程式設計成「像 Excel 那樣運作」，而不是靠工程師寫一堆「手動更新的程式碼」。

|關鍵概念|意義|在 database inside-out 的角色|
|---|---|---|
|UNIX pipe 式 dataflow|每當資料變動，就自動觸發後續處理|解耦處理邏輯，形成模組化系統|
|Database Inside-Out|拆分資料庫的內建功能，用外部組件取代|讓不同元件可以以資料流方式連接|

📌 **簡單來說**：

> 如果你把資料庫拆成很多小功能模組（inside-out），你就會需要一個能自動連接、觸發這些模組的方法（dataflow），這就像 UNIX pipe 一樣的設計哲學。



#### Application code as a derivation function
所有的衍生資料（derived data），本質上都是原始資料經過「轉換函數」後的結果。這個轉換函數（derivation function）通常會由應用程式碼負責實作。


#### Separation of application code and state
- **應用邏輯交給應用伺服器跑**（例如跑在 Docker、Kubernetes 上）
- **資料交給資料庫管理與持久化**（例如 PostgreSQL、MongoDB）

#### Dataflow: Interplay between state changes and application code
當我們把資料系統設計成 dataflow 模式，應用程式就不再是主動操控資料的「司令」，而是根據資料改變被「觸發」的一員。

#### Stream processors and services
| **面向**  | **Microservices** | **Dataflow**   |
| ------- | ----------------- | -------------- |
| 資料取得    | 即時請求（REST / RPC）  | 預先訂閱資料流        |
| 錯誤處理    | 有外部依賴點，風險高        | 資料本地快照，風險低     |
| 效能      | 慢（受限於網路 / 對方壓力）   | 快（在地處理）        |
| 架構模型    | 拉（Pull-based）     | 推（Push-based）  |
| 時間一致性處理 | 複雜（需補做）           | 較自然（資料本身帶有時間軸） |

### Observing Derived State
![700](https://github.com/Vonng/ddia/raw/main/img/fig12-1.png)

#### Materialized views and caching
透過在write path中做更多工作，來減少read path所需要的時間

#### Stateful, offline-capable clients
我們可以想像客戶端的裝置中的狀態，就是server端的cache狀態。

#### Pushing state changes to clients
- 傳統 HTTP 是「請求→回應」，如果你不刷新網頁（重新請求），資料就**永遠停在那一刻**。
- RSS 這種也是「不斷輪詢」的變體（polling），沒什麼智慧。
- 現在有了進步的方式，例如：
    - **WebSocket**
    - **Server-Sent Events (SSE)**
- 這些都可以讓伺服器**主動推播訊息給前端瀏覽器**，即使使用者沒有再發起請求。
這就是把資料流（stream）**從伺服器內部一路延伸到用戶端**的實作！

#### End-to-end event streams
1. 現在前端（像 React, Redux, Elm）**本來就在使用 event stream 處理 UI 狀態**。
2. 為什麼不把這個 model 延伸到整個系統？
    - 比如：一台裝置上的操作觸發事件 → 傳到 server → 經過 processing → 另一台裝置上的 UI 馬上更新。
3. 像即時聊天、多人遊戲早就這樣做了。
4. 但現在主流 web 系統，**還是卡在 stateless client + request/response model**。
5. 真正要實現這樣的「全端事件流」，**需要徹底改變資料系統的建構方式**，導向 _publish-subscribe_ 的資料流架構。

#### Reads are events too
##### **傳統作法：**
- **寫入**走的是 event log → stream processor → materialized view（資料庫）
- **讀取**則是直接查這些 materialized view（像是打 API 查快取）
##### **作者提案的想法：**
- 不如 **讀取請求本身也當成一個事件**，進到 stream processor 中
- stream processor 可以依照現在狀態 **即時回應這個讀事件**

就像是：
- 購物網站的「查詢庫存」請求
- 金融網站的「查詢報價」請求
這些其實可以當成一個「read event」，送進系統，用 stream processor 處理回應。

#### Multi-partition data processing
|**面向**|**傳統分散式資料庫 (如 MPP)**|**Stream Processor 處理多 partition 查詢**|
|---|---|---|
|易用性|內建支援 join、shuffle 等邏輯|要自己實作 join 邏輯、組成查詢流|
|性能|最佳化查詢引擎，專為 OLAP 設計|更靈活、可因應高吞吐需求場景|
|適合情境|預測型分析、大型 batch 查詢|實時風控、社群行為統計、微服務中的決策邏輯|
|架構風格|一體化系統（整合式 DB）|組合式系統（unbundled, 可擴展）|
- **查詢也能當成一種 event**：可放進資料流系統中處理
- **處理跨 partition 資料時很有幫助**：查詢可以被分發、執行、聚合
- **特別適合大型應用**：例如社群分析、風控系統、遊戲後台事件處理等
- **跟傳統 MPP 資料庫功能相似，但更靈活**


## Aiming for Correctness

- **Stateless service（無狀態服務）壞了，沒差。**
    - 例如一個 UI 前端、或是查資料的 API，只要重啟就好。
    - 不會影響「資料本身」。
- **Stateful system（有狀態系統）壞了，問題會留下來。**
    - 資料庫、分散式快取、事件記錄這些都要儲存資料。
    - 如果寫錯了或資料壞了，那這個錯就會「存進歷史」，之後分析、追蹤、修復都很麻煩。
- **我們一直靠「ACID」來保證 correctness，但實務上這一套不夠。**
    - 很多資料庫聲稱自己支援一致性（consistency）、隔離等，但其實實作不可靠。
    - Jepsen 測試就揭露了很多資料庫在斷線、錯誤時會失控。
    - 程式本身也很容易用錯，特別是遇到「弱隔離」、「非同步複寫」、「分區容錯」等進階設定。
- **你可以選擇「相信運氣」，也可以選擇「正確性保證」。**
    - 相信運氣：簡單、快、但可能資料亂掉。
    - 正確性保證（如 serializability、atomic commit）：資料正確，但無法橫跨地理位置，且效能差。


### The End-to-End Argument for Databases

#### Exactly-once execution of an operation
即使系統出錯、訊息重送，**最終效果就像這個操作只執行過一次一樣**。

讓操作變成 _Idempotent_（冪等性）


#### Duplicate suppression
##### 重複抑制（Duplicate Suppression）的挑戰

| 層級   | 重複抑制方式             | 侷限與風險                       |
| ---- | ------------------ | --------------------------- |
| TCP層 | sequence number 去重 | 僅限單一 TCP 連線，跨連線無法保證         |
| 資料庫層 | 2PC 可以保證交易一致       | 但無法防止 client 因 retry 重複送出操作 |
| 應用層  | 檢查操作 ID、token 等機制  | **最關鍵的一層**，也是最容易被忽略的一層      |

#### Operation identifiers
| **核心要點**            | **解釋**                               |
| ------------------- | ------------------------------------ |
| 操作識別碼（operation ID） | 每筆用戶操作需有唯一編號，防止重複執行                  |
| End-to-end flow     | 此識別碼應從 client 傳到後端資料庫，全鏈追蹤           |
| 資料庫層級防重             | 使用 UNIQUE constraint 在交易內中斷重複操作      |
| 結構設計演進              | requests 表可視為事件來源（event sourcing 基礎） |

#### The end-to-end argument
| **主題**               | **說明**                                          |
| -------------------- | ----------------------------------------------- |
| End-to-End Principle | 只有應用層有語意資訊，系統核心應該由兩端協調，不能只靠中間協議處理語意錯誤           |
| 通訊層能力限制              | 中間層（TCP、HTTP）只能提供**不完整、抽象的傳遞保證**，但無法保證語意不重複、不出錯 |
| 實作責任歸屬               | 操作是否成功、重送是否合理，**只有應用本身能判斷**                     |
| 不完整方案的用途             | 雖然有些機制能在效能上幫助（如暫存、QoS、ack），但不能完全依賴來保障語意正確性      |
> 通訊系統就像一條山洞，它能防止中途的監聽與干擾（像 WiFi 密碼或 TLS），但無法決定訊息的意義是否正確處理。真正能保護語意正確性的，是山洞兩端的人，也就是應用程式本身。

We just need to remember that the low-level reliability features are not by themselves sufficient to ensure end-to-end correctness.


#### Applying end-to-end thinking in data systems

作者認爲我們應去探索fault-tolerance的應用層面端對端抽象（application-specific end-to-end abstraction），並同時保持效能。

### Enforcing Constraints

#### Uniqueness constraints require consensus

不可使用 async multi-master replication
這句非常關鍵，意思是：
##### 如果你是用「多主架構 + 非同步複製」的方式（例如 Cassandra、Dynamo-style DB），就**無法保證唯一性**，因為**多個節點可能會同時接受相同值的寫入**。
📌 舉例：
- 同一時間，兩個節點都接受「註冊 user: alice」
- 等同步複製時才發現撞名，已經來不及了（除非你允許最終一致，但那不符合 uniqueness）

#### Uniqueness in log-based messaging
| **元素**                         | **作用**                            |
| ------------------------------ | --------------------------------- |
| log + partition                | 確保「有可能衝突」的操作都被送往同一個處理單位           |
| single-thread stream processor | 決定順序 → 抑制重複操作（例如 username 搶註冊）    |
| output stream                  | 回傳結果給 client                      |
| 本質上                            | 這就是 total order broadcast → 等價於共識 |

#### Multi-partition request processing
|   |   |
|---|---|
|request_id|end-to-end idempotency 標記，支援 deduplication|
|log append|單一寫入原子性保證，確保 request 有 or 沒有|
|stream processor crash 復原|deterministic 處理，重跑也不出錯|
|partitioned instruction streams|每個帳戶可獨立處理，不需全局鎖|
|local state|每個 processor 可以自己追蹤帳戶餘額，支援語意檢查（例如不透支）|

|**情境**|**為什麼不用 2PC 而用 event log**|
|---|---|
|訂單 → 訊息通知|訊息可能失敗或延遲，使用 outbox 可復原|
|支付成功 → 給使用者加點數|使用 event log 傳遞 PaymentConfirmed 事件再由點數服務處理|
|註冊帳號 → 發送歡迎信|寫入帳號表 + 寫入 outbox 中的 UserRegistered 事件，後台再送信|
|訂單成立 → 扣庫存|由庫存服務訂閱 OrderCreated，處理後自行更新庫存，實現 eventual consistency|

- **一般應用系統** → 接受 eventual consistency，就像「人事系統晚點才更新薪資也沒人死」
- **金融系統核心部分** → 要求 strict consistency，就像「醫院不容許病歷同步失敗」

### Timeliness and Integrity
1. **一致性（consistency）這個字眼太模糊了**，所以要拆成：
    - **Timeliness（是否夠即時）**
    - **Integrity（是否夠正確）**
2. **event-driven 架構能犧牲 timeliness 但要保住 integrity**
    - ex: outbox pattern ➜ 資料晚發，但不遺失
    - ex: deduplication ➜ 重送沒關係，但不能亂扣錢
3. 當你做架構設計時，應該先問自己：
    **這個業務場景，我最不能接受的是「資料來晚」，還是「資料錯了」？**

#### Correctness of dataflow systems
只要你能容忍非即時（Timeliness），那麼 event log 架構是你未來應用系統的最佳選擇之一

#### Loosely interpreted constraints
很多商業邏輯上，我們其實是可以容許Timeliness的。例如機票或是旅館其實是容許超額訂位的，因為他們預期有些客戶會取消訂單。
因此也會有所謂的補償、道歉措施，而且代價通常也不會太高。
我們仍需要Integrity，但很多事情是可以事後補救的。

#### Coordination-avoiding data systems

> [!NOTE]
> ## **⚖️ 最後的比喻與權衡思維：**
> ## **“道歉模型”**
> 這是一個非常有趣的觀點：
> > Coordination 與 constraint 的目的其實是 **減少 inconsistent 狀態造成的 “道歉”**
> > 但 Coordination 也會增加 **因為系統延遲 / 故障而需要道歉的次數**
> > 🎯 所以你的設計目標，不是零道歉，而是：
> > 👉 **找到「不會過度犧牲 availability，又能保住合理一致性」的甜蜜點（sweet spot）**
> 這正是「可接受的一致性模型設計」的現代實踐哲學。

如果你能接受 eventual consistency，但不能接受 data corruption，那麼你應該考慮採用 coordination-avoiding、event-driven 的資料流系統架構。它能讓你在不犧牲資料正確性的前提下，避開分散式交易地獄，獲得更好的性能與彈性。


### Trust, but Verify
就算是記憶體，偶而也會出現bit flips這種現象。
重點是，不要完全假設極度可靠的東西（記憶體）會永遠不會出錯。
#### Maintaining integrity in the face of software bugs
無論你使用多麼強大的資料庫、設計多麼嚴謹的交易系統，只要應用層或底層有 bug，你的資料完整性仍然可能會被破壞。

#### Don’t just blindly trust what they promise
#### A culture of verification
#### Designing for auditability
|**類型**|**傳統 Transaction DB**| **Event Sourcing**  |
| --------------- | --------------------- | ------------------- |
|資料可追溯性|限於低階資料操作| 明確紀錄 user intent    |
|可重建性|難以還原變動邏輯| 可重播 event，rebuild   |
|除錯能力|靠 logs 與人工分析| 可追蹤事件 → 狀態變動        |
|整體正確性保障|靠 ACID，黑盒推論| 可白盒還原，強 integrity   |
|integrity check|幾乎無法事後驗證| replay-based 驗證<br> |

#### The end-to-end argument again
資料完整性（Integrity）必須用「End-to-End」的方式來檢查與維護，才能有效防止資料腐敗（corruption）在系統中悄悄蔓延。

#### Tools for auditable data systems
將這些 cryptographic audit 技術應用在一般資料系統中，**可能會是未來趨勢**。
雖然目前這類技術的效能與可擴展性還不如傳統系統，
但若能改善效能，**有潛力大幅提升資料正確性的信心與可驗證性。**


## Doing the Right Thing

### Predictive Analytics
談了像是歧視、因為你和誰相似就認定你信用不好等系統，可能會陷入讓有錢人更有錢，把窮人推向地獄的循環。
而我們應該要懷有道德的去審視我們的系統。

### Privacy and Tracking