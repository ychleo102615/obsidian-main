---
tags: []
date: 2025-03-27
time: 18:39
---
replication 難在資料是會變動的。

## Leaders and Followers
leader會首先被寫入，再由leader推送replication log or change stream給followers。

## 同步與非同步
leader需要等待被設定為同步的follower。


## Handling Node Outages
一個node會因為意外或是刻意的觸發而失效。我們希望可以重啟個別node without downtime。
#### Follower failure: Catch-up recovery

#### Leader failure: Failover
指配新leader，通知client新寫入的對象，更新其他follower追隨的對象。
潛在問題：
1. 新leader可能沒完全同步舊server的寫入。通常舊leader就得捨棄這些內容
2. 在於外部儲存系統合作（redis ）時，這種捨棄可能會造成問題
3. 可能有兩個node都相信自己是leader
4. 挑選適合的timeout時長


## Implementation of Replication Logs
#### Statement-based replication
 For a relational database, this means that every INSERT, UPDATE, or DELETE statement is forwarded to followers
#### Write-ahead log (WAL) shipping
在修改資料之前，先把這個修改記錄寫入一個日誌（log）中。
這通常是binary的，等於與storage engine綁死，
當執行failover時，WAL可能會造成downtime，因為新舊版leader版本不同。

#### Logical (row-based) log replication
與storage engine解耦
就算新舊leader是不同版本或storage engine，也有辦法前後相容。

#### Trigger-based replication
設定條件，當符合條件的資料更新時，就會觸發，可以寫到其他地方或是傳送給其他人。
但相當消耗效能。

## Problems with Replication Lag

### Reading Your Own Writes
使用者對leader寫入後，並從尚未更新的follower讀取，會發現剛剛寫的東西沒有在上面。
面對這種情況，我們需要read-after-write consistency, also known as read-your-writes consistency。

#### When reading something that the user may have modified, read it from the leader; otherwise, read it from a follower.
例如使用者自己的檔案頁面，幾乎只有可能是使用者自己編輯的

#### 按照最後的update時間決定要向leader還是follower query
適合用在所有東西都能被使用者編輯的狀況

#### Client紀錄最後編輯的時間戳記
避免像還沒跟上該戳記的replica索取資料，或是等到query 的 replica 追上這個戳記
但多個裝置時會有問題


### Monotonic Reads
當向不同replica讀取時，可能會發生時間倒退的資料，因為後來讀取的replica進度比較慢。
只向同一個replica讀取可以解決問題。但還是要考慮這個replica掛了的情況。

### Consistent Prefix Reads
This guarantee says that if a sequence of writes happens in a certain order, then anyone reading those writes will see them appear in the same order.
 This is a particular problem in partitioned (sharded) databases.

我們不會想看的這樣的對話順序：
> Mrs. Cake
> About ten seconds usually, Mr. Poons.
> Mr. Poons
> How far into the future can you see, Mrs. Cake?

**Consistent Prefix Reads** 強調的是「讀取時要看到從歷史起點開始的連續事件，不可以跳過中間某些步驟」，這樣才能保證資料的合理性與因果順序。

|詞|重點|
|---|---|
|**Prefix**|從開頭開始的一段連續事件|
|**Consistent**|沒有中斷或跳躍，符合因果順序|

### Solutions for Replication Lag
Pretending that replication is synchronous when in fact it is asynchronous is a recipe for problems down the line.

雖然應用程式可能更有辦法來處裡延遲，但難免變得複雜且容易出錯。
最好還是讓database來處裡這種底層問題，讓責任分離。
而這也是為什麼會有transaction 存在。

## Multi-Leader Replication
allow more than one node to accept writes.

### Use cases for Multi-Leader Replication
#### Multi-datacenter operation
一個datacenter可以有自己的leader。其中一個datacenter down了互不影響。


但容易有很多問題
- 兩個寫入有可能互相衝突
- 這像是加裝在database上的功能，與其他database互動時容易產生問題

少用

#### Clients with offline operation
像是日曆的應用程式，我們要能隨時使用。
所以裝置本身就會是一個leader，如果有多個裝置的話，就是multi-leader replication了。

#### Collaborative editing
例如Google文件允許多人同時編輯。
雖然我們不會說這是一個database replication的問題，但他與offline editing 很像。

### Handling Write Conflicts

^24e0b7

這種問題不會出現在single-leader database。

#### Synchronous vs asynchronous conflict detection
同步衝突偵測，其實還不如用single-leader replication

#### Conflict avoidance
以編輯個人頁面的應用程式為例，我們可以總是讓同一名使用者使用同一個leader。
但是今天該leader down，或是地區轉移了，就還是可能要面對衝突。

#### Converging toward a consistent state
Database must resolve the conflict in a convergent way, which means that all replicas must arrive at the same final value when all changes have been replicated.
資料庫必須以收斂（convergent）的方式解決衝突，也就是說，當所有變更都完成複製後，所有副本必須達到相同的最終值。

- 給每個寫入UUID，讓比較大的值為王。或是依照時間戳讓最後一個寫入為王。但是容易造成data loss
- 讓replica有排序，讓高排序的能覆寫低排序。同樣有data loss
- 直接合併，例如”B/C”
- 以某種方式標記，讓應用程式之後處理

#### Custom conflict resolution logic
讓應用程式處理，時機可能是onWrite或onRead。
一個transaction可能包含多個write，算個別的衝突。

#### Automatic Conflict Resolution
- Conflict-free replicated datatypes (CRDTs)   可被併發編輯的資料結構
- Mergeable persistent data structures   類似git
- Operational transformation   併發編輯  googleDoc

### Multi-Leader Replication Topologies
多leader可以有不同種互相傳播的方式：環狀、星狀、網狀all to all
前兩者容易有failure問題
網狀會有overtake問題


## Leaderless Replication
也被稱為Dynamo-style

### Writing to the Database When a Node Is Down
沒有leader就沒有所謂的failover。

今天一個 node停機時，其他 node有所改動。當這個node恢復時，他的資料是stale的。

#### Read repair and anti-entropy
- read repair  read時，比對其他node的版本，將新版資料寫到。舊版replica
- anti-entropy 背景執行整份複製

#### Quorums for reading and writing
$w + r > n$
保證寫入數 + 最低讀取數 > 總 nodes數
這樣能夠保證至少有一個node是最新的
常見的做法是讓n是奇數，並且讓w, r都過半。


> [!note]- partitioning
> 叢集中的節點數量可能超過 n 個，但任何一筆資料（value）只會被儲存在其中 n 個節點上。
> 這樣的設計讓資料可以被分區（partitioned），因此能支援比單一節點容量更大的資料集。
> 
> ##### 關鍵概念說明：
> • n nodes（儲存副本的數量）：這是設定的「每筆資料的複本數」，例如 n = 3，代表每筆資料會有 3 份存在不同節點上。
> • 叢集（cluster）：整個系統中實際有的節點數可能大於 n，例如你有 10 台機器，但每筆資料只複製到其中 3 台。
> • 分區（partitioning）：不同的資料會被分散儲存在不同節點上（透過 consistent hashing 或其他方式），這樣整體資料集可以大於單一節點的容量限制。
> • 容錯與擴展性：儲存在 n 個節點上提供了容錯（例如其中某幾個節點壞掉不會影響資料）與水平方向擴展的能力。
> 
> ##### 舉個例子更好理解：
> 假設你有一個包含 1TB 資料的系統：
> • 每台機器最多只能儲存 300GB，那 1TB 根本放不下。
> • 你用 5 台機器（叢集裡有 5 nodes）。
> • 系統設定為每筆資料複製到 n = 3 個節點上。
> • 系統會把資料切成多份（partition），讓不同的資料分布到不同的節點上。
> • 每筆資料會有 3 個副本，但整體資料卻能超過任何單一節點容量限制。


### Limitations of Quorum Consistency
即使我們設定了這個公式，實際上還是有很多可能讓我們拿到過時的資料。調整w, r頂多是調整讀到stale value的機率。
- sloppy quorum
- concurrent write
- write, read同時
- 寫入失敗時，寫成功的node沒有rolled back
- 回復node和其他edge cases

#### Monitoring staleness
對leader-based replication而言，只要量測leader, follower的寫入位置距離，就能知道replication lag。
但對leader-less來說就很難了，目前沒有common practice


### Sloppy Quorums and Hinted Handoff
因為quorum的特性，不用等所有人，只要有w, r個人回應就好。
這讓 leader‐less replication 適合 high availability and low latency的使用場景，並可容許偶發的過時資料。

**sloppy quorum**: writes and reads still require w and r successful responses, but those may include nodes that are not among the designated n “home” nodes for a value.
規定寫入的w數量，不用全部來自當初指定的n個node。

hinted handoff代表讓借住的資料返回他應該回去的地方。

#### Multi-datacenter operation
leaderless 的架構適合處理multi-datacenter，他本來就被設計成處理衝突與網路中斷。

寫入同樣是推送給所有datacenter，我們可以設計quorum只需要滿足local datacenter就好。
跨datacenter的寫入會在背景非同步執行。 

### Detecting Concurrent Writes
當對多個client對同一個key值同時寫入，我們都只採取overwrite的話，資料將會permanently inconsistent。
參考[[5. Replication#Handling Write Conflicts]]

#### Last write wins (discarding concurrent writes)
一個簡單的方法是讓最後寫入的覆蓋。但我們很難知道誰先誰後（範例中，不同的node，比較早的寫入是不同的），我們稱此狀態concurrent 。
我們需要多加時間戳來比較。

但這是很暴力的做法。其他寫入被無聲拋棄，且有時甚至會拋棄不是concurrent的寫入。
少數像是快取，這樣的方式是可接受的。否則大部分的時候LLW都不是解決衝突的好選擇。

其他能保證安全的情況，就是資料屬於不可變的，或是key是global unique。

#### The "happens-before" relationship and concurrency
concurrency並不一定要同時發生。
兩個寫入A, B，若A知道B(B先於A)或是B知道A，代表他們不是併發的。反之，他們彼此互相不知道對方，那就是併發了。
##### 相對論的聯想
講的極端一點，若AB之時間差小於他們之間的距離除以光速，AB是無法互相影響的。


#### Capturing the happens-before relationship
```python
# 1
client1 寫入 [milk]，回傳 ver.1 [milk]
# 2
client2 寫入 [eggs]，回傳 ver.1 [milk], ver.2 [eggs]
# 3
client1 寫入 ver.1 [milk, flour]，server覆寫ver.1，回傳 ver.2 [eggs], ver.3 [milk, flour]
# 4
client2 合併 ver.1, ver.2 ，寫入ver.2 [eggs, milk, ham]，server覆寫ver.2，回傳 ver.3 [milk, flour], ver.4 [eggs, milk, ham]
# 5
client1 合併 ver.2, ver.3，寫入ver.3 [milk, flour, eggs, bacon]
```

#### Merging concurrently written values
Riak calls these concurrent values *siblings*.
two final siblings are `[milk, flour, eggs, bacon]` and `[eggs, milk, ham]`;
採取union的做法的話，可以得到`[milk, flour, eggs, bacon, ham]`，但是union無法處理移除的情況。

我們會需要加入一個deletion marker，又叫做*tombstone*。

合併的方式很複雜，參考[[5. Replication#Automatic Conflict Resolution]] CRDT

#### Version vectors
每個replica，每個key都需要有version number。
這使我們能夠決定哪些值要被覆寫，哪些值是siblings。
這些version numbers被稱為 version vectors。