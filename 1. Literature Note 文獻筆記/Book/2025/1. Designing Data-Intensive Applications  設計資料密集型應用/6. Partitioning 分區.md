---
tags: 
date: 2025-04-20
time: 23:51
---

術語
partition, shard, region, tablet, vnode, vBucket
這些只的都是一樣的概念，只是在不同的資料庫服務上有不同的稱呼

本章首先會看：
1. 如何分割，並且資料的索引會如何與分割互相影響
2. 平衡，加入或移除node
3. 如何引導request到對的partition

## Partitioning and Replication
通常partitioning會與replication一起使用。
所以就算一筆資料只屬於一個partitioning，他也會出現在多個replication中。

一個node可以儲存多個partition。可能會長得像這樣。
```
Node 1 (par.1 - leader,   par.2 - follower)
Node 2 (par.1 - follower, par.2 - leader)
```

在第五章討論的replication內容，也會同等的反應在每一個partition。
partition的決策是可以與replication相互不影響的。

## Partitioning of Key-Value Data

我們該怎麼決定哪些資料要存在哪些node上面？

我們的目標是要盡可能保持資料和索取的量要平衡。理想狀況下，10個node就能提升10倍的資料以及吞吐量。
如果分配不均，使某node的load太多，我們稱之為 *skewed*，而該partition則叫他 *hot spot*。

一個簡單的方法是把資料隨機分佈，但由於這樣無法得知哪個資料在哪個partitioning，所以在索取的時候要向所有的nodes執行。
我們有其他更好的方法。

### Partitioning by Key Range
這就像是按照字母排序的百科全書，第一本包含A開頭到B開頭的字，第二本包含B開頭到D開頭的字．．．。而每一本書就像是一個node。
這些分界可以由管理員手動設定，也可以讓資料庫自動選擇（之後還有rebalance的議題）。
這種排序在索取串聯的資料（例如索取key值為一定範圍的時間的量測資料）會很好用。

但是壞處就是，很容易讓一個partition成為hot spot，以上面的時間為例，今天partition就會一直被寫入。
我們可以在key值前面加入其他前綴來避免這個狀況，例如將量測器的編號設定為前綴。

### Partitioning by Hash of Key
我們希望將任何資料丟入hash function後，得到非常平均分布的結果。因為是以paritition為目的，不太需要加密上的強度。
如此一來，我們可以將一個範圍的hash導引到一個partition。

這裡有提到一個叫consistent hashing 的技術，一種用來平均分配網路上的吞吐，例如CDN content delivery network。

但是使用hash的話，我們失去可以用範圍來索取資料的優勢。

Cassandra有一個折衷方案，讓一組compound primary key的第一個column作為hash，其餘作為concatenated index。
例如：
 `(user_id, post_timestamp)`
- `user_id` ➜ 哪個使用者發的貼文？（分區 key）
- `post_timestamp` ➜ 該使用者貼文的時間（排序 key）
#### 🧩 為什麼這個結構適合「一對多」？
因為：
- 一個 `user_id` 對應 **多筆貼文（many posts）**
- 所有屬於該使用者的貼文：
    - 被儲存在**同一個 partition（同一台機器）**
    - 而且依照 `post_timestamp` 排序
- 所以查詢這個使用者最近的貼文、或某段時間內的貼文，就會非常快！

這讓我想到[[355. Design Twitter]]這題，有著很像的結構，只是尺度擴大到database了。

### Skewed Workloads and Relieving Hot Spots
雖然hash本身可以降低hot spot 發生的機會，但他無法完全避免。
例如名人或是熱門的對話串。這種情況目前通常不是靠資料庫，而是靠應用程式來處理。（或許未來有機會使資料庫自動處理）

一個簡單的方式是把該hot key加入兩個數字，這樣就可以分散到100個不同的地方了。當然，這對吞吐量小的使用情境是overhead。

###### 補充：
- ✅ **Hash partitioning** 通常預先建立大量固定 partitions，是為了避免之後改 partition 數時造成大規模 rehash。
- ✅ **Range partitioning** 可以隨著資料分布「動態切割」partition，因此 **不需要固定數量**，而是根據資料分布調整。

## Partitioning and Secondary Indexes
secondary index通常是用來過濾搜尋資料的。
secondary index 不會直接對到該partition。我們有兩種主要的處理方式。

### Partitioning Secondary Indexes by Document

##### 書中對 "Document" 的定義簡單補充：
- 一筆資料的單位，可能是 user profile、product、log entry…等等。
- 通常是以 JSON、BSON、XML 或類似格式儲存的結構化資料。
- 在 **document-oriented database（文件導向資料庫）** 中，像 MongoDB 就是這樣的設計，每筆 document 有一個 `_id`（主鍵），其他欄位像 `color`, `make`, `location` 就是可以被索引的次欄位。

Partitioning by Document 是什麼意思？
它的意思是：
將次索引也跟著主資料（document）一起分在相同的 partition 裡。
也就是說，如果某筆 document 被放在分區 A，那它的 secondary index 條目也只存在於 A，不會集中存一個跨所有分區的全局索引。
又稱local index。


| 優點                | 缺點                   |
| ----------------- | -------------------- |
| index 跟資料放一起，更新容易 | 查詢要掃過所有分區，慢          |
| 寫入快，無需跨機器同步       | 不適合頻繁查詢次索引欄位         |
| 可用單機事務保證一致性       | scatter/gather 查詢開銷大 |

### Partitioning Secondary Indexes by Term
又稱global index。
我們不會把所有的index存在一個node，而是也會跟著被partitioned。
我們叫這種方式*term-partitioned*，因為我們尋找的term決定了他在哪一個partition。
term可以是出現在document中的所有文字。

term-partitioned 會比 document-partitioned 更有效率的讀取，但相反的事寫入的時候較慢且複雜，因為你同時要寫入多個partition。

## Rebalancing Partitions
把負載移動到其他node的行為就叫做 *rebalancing*。

rebalancing需要滿足以下需求
- 再平衡之後，每個node之間的負載會變得差不多
- 再平衡期間，能夠繼續讀取與寫入
- 不移動沒必要移動的資料

### Strategies for Rebalancing

#### How NOT to do it: hash mod N
使用mod時，例如本來資料是存在編號6的node上，但是因為hash(key)改變的話，他可能需要移動到其他編號的node上，那這就屬於沒有必要移動的資料。

#### Fixed number of partitions
建立比nodes數量還要多的partitions。例如在有10個node的database中建立1000個partitions。

新增或是移除node的時候，可以以partition為單位與其他node之間rebalance資料。
partition的總數量與key都不會改變。會變的只有與node與partition對應的關係。

你需要確保partitions的數量夠高，因為這同時就是你node的最高數量，要能夠對應未來的成長。
同時，這個數量太高也不好，會有更多的隱形成本。

太多導致rebalancing, recovery太花費資源，太少也容易導致其他開銷。最重要的還是剛好。
資料的的大小容易變動的話，就會很難抓這個數字。

#### Dynamic partitioning （partition的尺寸都會差不多）
對Partitioning by Key Range來說，固定的partition 和其邊界就會非常不方便。

dynamic partition可以在partition的資料太多時分成兩個，或者太少時與其他partition合併。這樣的行為與B-Trees很像。
最大的partition會被設定成固定的。
但初始的時候只有一個partition的話，可能會讓其他node都處於閒置狀態，所以有些database可以一開始就設定一組partitions在空的database上（這叫做pre-splitting）。
除了key-range之外，也適合用在hash-partitioned data。


#### Partitioning proportionally to nodes （每個node會有的partition數量固定）
dynamic partitioning時，partition的數量會隨dataset按比例成長，只要我們設定partition的size有個固定範圍（以執行split, merge）。
反之，我們固定partition的數量的話，partition的size就會按比例成長。
這兩個情況，partition的數量都與node的數量無關。

第三種方式，則是按照nodes數量成長。
如果node數量不變，那partition的數量就不會動，partition size會按比例成長。node數量增加，partition size則會變小。

當加入一個新node時，他隨機題選一定數量個partition去split，將一半分給自己，另一半留在原地。如果這個數量夠大的話，就不容易出現unfair splits。

|策略|缺點|適用場景|不適用場景|
|---|---|---|---|
|**固定分區數＋動態映射**（Fixed partition count）|- 事先要決定分區總數，若估太少，節點變動時仍要搬動大量分區；估太多，平日維護 metadata 負擔增大。- 分區數固定不隨資料量增長，熱點資料仍需應用層額外處理（如隨機前綴） ([mrnice.dev](https://www.mrnice.dev/posts/designing-data-intensive-applications-bookclub-flashcards/?utm_source=chatgpt.com "Designing Data-Intensive Applications: Flashcards for a Book Club"))|- 節點增減頻繁，但資料量成長平緩- 想以最小搬移開銷平滑擴縮- 系統已有灰度隨機鍵或前綴消除簡單熱點|- 資料量波動極大、熱點多- 分區預估困難，難以一次設定合理分區數時|
|**動態範圍拆分／合併**（Dynamic Range Splitting）|- 拆分／合併操作涉及同步鎖定或 snapshot 機制，可能短暫阻塞該分區的寫入或查詢。- 分區邊界頻繁調整，metadata 更新與路由協調開銷高 ([mrnice.dev](https://www.mrnice.dev/posts/designing-data-intensive-applications-bookclub-flashcards/?utm_source=chatgpt.com "Designing Data-Intensive Applications: Flashcards for a Book Club"))|- 資料呈明顯熱點（特定範圍寫入集中）- 需高效範圍查詢且希望自動消彌熱點- 範圍鍵（如時間序列、ID 順序）是主要查詢依據|- 寫入延遲極度敏感，無法容忍短暫寫阻塞- 範圍查詢比例低，熱點不常發生|
|**按節點比例動態調整**（Proportional to Nodes）|- 複合了前兩者的複雜度：既有拆分／合併，又要隨節點變化調整閾值，實作與測試難度高。- 若節點快速增減，拆／合併頻率高，容易造成系統抖動（jitter） ([mrnice.dev](https://www.mrnice.dev/posts/designing-data-intensive-applications-bookclub-flashcards/?utm_source=chatgpt.com "Designing Data-Intensive Applications: Flashcards for a Book Club"))|- 節點與資料量同時大幅波動- 需在節點增減時同步維持每分區負載穩定- 希望減少手動調節與預估誤差|- 系統簡單、節點少且變動不頻繁- 不需要極度嚴格的每分區大小平衡時|

### Operations: Automatic or Manual Rebalancing
我們可以在完全自動與完全手動之間找一個適合程度來做rebalancing。
完全的自動雖然很方便，也比較不需要維護，但是他會比較不可預測。
例如當一個node已經overload了，結果現在還自動做rebalancing，這會使情況變得更糟。

## Request Routing
我要怎麼知道現在的key，要往哪個IP, 哪個port找？
這是一種常見於網路服務的問題： *service discovery*

有幾種方法
- 允許request找到任何node。如果要找的partition就在目前的node，那就直接處理該request；否則這個node要負責送出request到下一個node並回傳結果
- 送request到一個routing tier，partition-aware load balancer
- client要自己知道partition在哪個node

這是很困難的問題。許多分散式系統會依賴於一個額外的coordinate service 像是 ZooKeeper。他會負責將「哪個partition在哪個node」的資訊發派到有訂閱他的actor。

### Parallel Query Execution
Massively parallel processing (MPP)
這是另一個專門的主題。要看Chapter10