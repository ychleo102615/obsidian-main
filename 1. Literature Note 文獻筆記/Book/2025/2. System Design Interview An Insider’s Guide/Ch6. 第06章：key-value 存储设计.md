---
tags: 
date: 2025-06-29
time: 21:06
link: https://github.com/Admol/SystemDesign/blob/main/CHAPTER%2006：DESIGN%20A%20KEY-VALUE%20STORE.md
---
### 单一服务器的键值存储


开发一个驻扎在单个服务器中的键值存储很容易，一种直观的方法是将键值对存储在哈希表中，该哈希表将所有内容保存在内存中。

为了在一个服务器中容纳更多的数据，可以做两个优化措施：

```
- 数据压缩
- 只在内存中存储经常使用的数据，其余的存储在磁盘上
```

即使进行了这些优化，单个服务器也可以很快达到其容量。需要分布式键值存储来支持大数据

### 分布式键值存储


分布式键值存储也称为分布式哈希表，它将键值对分布在许多服务器上。在设计分布式系统时，了解 CAP（C一致性、A可用性、P分区容错性）定理很重要。

CAP 定理指出，分布式系统不可能同时提供以下三种保证中的两种以上：一致性、可用性和分区容错性。让我们熟悉一些定义。

**一致性**：一致性意味着所有客户端无论连接到哪个节点，都在同一时间看到相同的数据。

**可用性**：可用性意味着即使某些节点已关闭，任何请求数据的客户端都会得到响应。

**分区容忍度**：分区表示两个节点之间的沒有連線，分区容错意味着系统在网络分区的情况下继续运行。

CAP 定理指出，必须牺牲三个属性之一来支持 3 个属性中的 2 个，如图 6-1 所示：

如今，键值存储根据它们支持的两个 CAP 特性进行分类：

![500](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-1.jpg)

**CP（一致性和分区容错）系统**：CP 键值存储在牺牲可用性的同时支持一致性和分区容错。

**AP（可用性和分区容错）系统**：AP 键值存储支持可用性和分区容错，同时牺牲一致性

**CA（一致性和可用性）系统**：CA 键值存储支持一致性和可用性，同时牺牲分区容错性。由于网络故障是不可避免的，分布式系统必须容忍网络分区。因此，CA 系统不能存在于现实世界的应用程序中。

您在上面阅读的内容主要是定义部分。为了更容易理解，让我们看一些具体的例子。在分布式系统中，数据通常会被复制多次。假设数据被复制到三个副本节点n1、n2和n3上，如图6-2所示。



在分布式系统中，分区是不可避免的，当出现分区时，我们必须在一致性和可用性之间做出选择。图6-3中，n3宕机，无法与n1、n2通信。如果客户端将数据写入 n1 或 n2，则数据无法传播到 n3。如果数据写入 n3 但尚未传播到 n1 和 n2，则 n1 和 n2 将具有陈旧数据。
![500](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-3.jpg)

如果我们选择一致性大于可用性（CP系统），我们必须阻止所有对n1和n2的写操作，以避免这三个服务器之间的数据不一致，这使得系统不可用。银行系统通常有极高的一致性要求。例如，对于银行系统来说，显示最新的余额信息是至关重要的。如果由于网络分区而发生不一致，在不一致问题解决之前，银行系统会返回一个错误。

然而，如果我们选择可用性大于一致性（AP系统），系统就会一直接受读取，即使它可能返回陈旧的数据。对于写，n1和n2将继续接受写，当网络分区解决后，数据将被同步到n3。

选择正确的 CAP 以确保适合你的用例是构建分布式键值存储的重要一步。你可以与面试官讨论这个问题并相应地设计系统



### 系统组件


在本节中，我们将讨论以下用于构建键值存储的核心组件和技术：

- 数据分区
- 数据复制
- 一致性
- 不一致解决方案
- 故障处理
- 系统架构图
- 写入路径
- 读取路径

下面的内容主要基于三个流行的键值存储系统：Dynamo 、Cassandra  和 BigTable 。


### 数据分区
參考上一章的一致性hash環，我們想要平均分配並減少資料的移動。
![400](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-4.jpg)

### 数据复制

为了实现高可用性和可靠性，必须在 N 个服务器上异步复制数据，其中 N 是一个可配置参数。这N台服务器的选择逻辑如下：将key映射到哈希环上的某个位置后，从该位置顺时针走，选择环上的前N台服务器存储数据副本。在图 6-5（N = 3）中，key0 被复制到 s1、s2 和 s3。

![400](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-5.jpg)



### 一致性

由于数据在多个节点进行复制，因此必须跨副本同步。Quorum 共识可以保证读写操作的一致性。 让我们先建立几个定义。

N = 副本数

W = 大小为 W 的规定写入。要将写入操作视为成功，必须从 W 个副本确认写入操作。

R = 大小为 R 的读取规定人数。为了使读取操作被认为是成功的，读取操作必须等待至少R个副本的响应。

- 如果R=1，W=N，系统被优化为快速读取
- 如果W=1，R=N，系统被优化为快速写入
- 如果W+R>N，就可以保证强一致性（通常N=3，W=R=2）。
- 如果W+R<=N，则不能保证强一致性
### 一致性模型


一致性模型是设计键值存储时要考虑的另一个重要因素。 一致性模型定义了数据一致性的程度，并且存在多种可能的一致性模型：

- 强一致性：任何读操作都会返回一个与最新的写数据项的结果相对应的值。客户端永远不会看到过期的数据
- 弱一致性：后续的读操作可能看不到最新的值。
- 最终一致性：这是弱一致性的一种特殊形式。只要有足够的时间，所有的更新都会被传播，而且所有的副本都是一致的。

强一致性通常是通过强迫一个副本不接受新的读/写，直到每个副本都同意当前的写来实现的。这种方法对于高可用系统来说并不理想，因为它可能会阻塞新的操作。Dynamo和Cassandra采用最终一致性，这是我们推荐的键值存储的一致性模型。


### 不一致的解决方法：版本控制
透過使用向量時鐘，偵測衝突。

尽管向量时钟可以解决冲突，但也有两个明显的缺点。 首先，向量时钟增加了客户端的复杂性，因为它需要实现冲突解决逻辑。

其次，向量时钟中的 `[server: version]` 对可能会快速增长。为了解决这个问题，我们为长度设置了一个阈值，如果超过了限制，则删除最旧的对。这可能导致协调效率低下，因为后代关系无法准确确定。然而，基于Dynamo论文，亚马逊在生产中还没有遇到这个问题；因此，这可能是大多数公司可以接受的解决方案。

### 故障处理

与任何大规模的系统一样，故障不仅是不可避免的，而且是常见的。处理故障情况是非常重要的。在本节中，我们首先介绍检测故障的技术。然后，我们将介绍常见的故障解决策略。

故障检测

低效的偵測方式：
![500](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-10.jpg)

一个更好的解决方案是使用分散的故障检测方法，如`gossip`协议。`gossip`协议的工作原理如下：

- 每个节点维护一个节点成员列表，其中包含成员ID和心跳计数器。
- 每个节点定期增加它的心跳计数器
- 每个节点定期向一组随机节点发送心跳，然后再传播到另一组节点上
- 一旦节点收到心跳，成员名单就会更新到最新信息。
- 如果心跳没有增加超过预定的时间，该成员被认为是离线的。

![800](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-11.jpg)


##### 处理暂时性故障

如果由于网络或服务器故障导致服务器不可用，将由另一台服务器临时处理请求，当宕机服务器启动时，更改将被推回以实现数据一致性。这个过程称为**暗示切换（hinted handof）**。由于图6-12中s2不可用，读写暂时交由s3处理，当 s2 重新上线时，s3 会将数据交还给 s2。

![500](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-12.jpg)

##### 处理永久性故障

- 這時候要靠 **Anti-Entropy Protocol**：
    
    > 主動比對副本之間的資料是否一致，並把「舊的或缺的」資料補上。
    

Anti-Entropy 如何執行？（用 Merkle Tree）

Merkle Tree（又叫 Hash Tree）幫助我們**有效率地比對兩個副本的差異**，重點如下：

###### 💡 Merkle Tree 是什麼？

- 是一種**二元樹結構**：
    
    - **葉節點**：是實際資料（或每筆 key 對應的 hash）
        
    - **非葉節點**：是其**子節點 hash 值的 hash**
        

> 這樣只要兩個節點的 root hash 相同，就能保證整棵樹（＝所有資料）相同。

###### 🔍 怎麼比對副本？

- 先比對兩棵 Merkle Tree 的 root hash
    
- 如果不一致，就往下一層（例如左子樹、右子樹）去比
    
- 最後可以精確地定位哪個 bucket 的資料不同 ➜ **只同步有差異的 bucket**
    

這樣做有兩大好處：

1. **減少傳輸資料量**（只同步有差異的部分）
    
2. **檢查速度快**（hash 很快比）

Merkle Tree 是一種高效的資料驗證工具，在分散式系統中用來進行資料同步與修復，只需比對 hash 就能知道哪邊資料不同，進而只同步有差異的部分，大幅減少成本。


### 系统构架图

![800](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-17.jpg)

架构的主要特点列举如下：

- 客户端通过简单的API与键值存储通信：`get(key)` 和 `put(key, value)`。
- 协调器是一个节点，在客户端和键值存储之间充当代理。
- 节点采用一致性hash的散列方式分布在一个环上。
- 该系统是完全去中心化的，所以添加和移动节点可以自动进行。
- 数据在多个节点上复制。
- 不存在单点故障，因为每个节点都有相同的职责。

![500](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-18.jpg)


### 写入路径

![600](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-19.jpg)

1. 写入请求持久保存在提交日志文件中。
2. 数据保存在内存缓存中。
3. 当内存缓存已满或达到预定义的阈值时，数据将刷新到磁盘上的 SSTable 。 注意：排序字符串表 (SSTable) 是 <key, value> 对的排序列表。


### 读取路径
![600](https://github.com/Admol/SystemDesign/raw/main/images/chapter6/figure6-21.jpg)

1. 系统首先检查数据是否在内存中。如果没有，就转到第2步。
2. 如果数据不在内存中，系统会检查Bloom过滤器。
3. Bloom过滤器被用来计算哪些SSTables可能包含密钥。
4. SSTables会返回数据集的结果。
5. 数据集的结果被返回给客户端。

### 总结


本章涵盖了许多概念和技术。为了加深记忆，下表总结了分布式键值存储的特点和相应的技术。

|目标/问题|技术|
|---|---|
|存储大数据的能力|使用一致性哈希将负载分散到多个服务器上|
|高可用性读取|数据复制 多数据中心设置|
|高可用性写入|使用向量时钟（vector clocks）进行版本控制和冲突解决|
|数据分区|一致性哈希|
|增量可扩展性|一致性哈希|
|异质性（heterogeneity）|一致性哈希|
|处理临时性故障|草率仲裁（sloppy quorum）和暗示切换（hinted handoff）|
|处理永久性故障|Merkle 树|
|处理数据中心中断|跨数据中心复制|